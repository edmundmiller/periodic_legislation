name: Run Pipeline Daily

on:
  schedule:
    - cron: '0 0 * * *'  # Runs every day at midnight UTC

jobs:
  run-pipeline:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

        # Stage previous data in
      - name: Install huggingface-cli
        run: pip install huggingface-hub

      - name: Authenticate with Hugging Face
        env:
          HUGGINGFACE_TOKEN: ${{ secrets.HF_TOKEN }}
        run: huggingface-cli login --token "$HUGGINGFACE_TOKEN"

      - name: Download Hugging Face Dataset
        run: huggingface-cli download edmundmiller/period_care_legislation --local-dir hf_stats --repo-type dataset --quiet

        # Run pipeline

      - name: Set up environment
        run: |
          sudo apt-get update
          sudo apt-get install -y jq duckdb

      - name: Run pipeline script
        env:
          APIKEY: ${{ secrets.APIKEY }}
        run: ./pipeline.bash

        # Stage bills.csv for upload

      - name: Copy bills.csv to Hugging Face Dataset
        run: cp bills.csv hf_stats/

      - name: Upload bills.csv to Hugging Face Dataset
        run: huggingface-cli upload edmundmiller/period_care_legislation hf_stats --repo-type dataset --quiet

      - name: Commit and Push Changes to Hugging Face
        run: |
          cd hf_dataset
          git config --global user.email "action@github.com"
          git config --global user.name "Periodic"
          git add bills.csv
          git commit -m "Update bills.csv"
          git push